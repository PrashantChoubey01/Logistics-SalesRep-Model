#!/usr/bin/env python3
"""
Response Validator Agent - Validates model responses against input queries

This agent ensures that the generated response:
1. Addresses the customer's query accurately
2. Contains relevant and accurate information
3. Is professional and appropriate
4. Matches the expected response type
"""

import json
import logging
from typing import Dict, Any, List, Optional

from .base_agent import BaseAgent

logger = logging.getLogger(__name__)


class ResponseValidatorAgent(BaseAgent):
    """
    Response Validator Agent - Validates model responses against input queries
    
    This agent uses LLM to validate that generated responses:
    - Address the customer's query accurately
    - Contain relevant information
    - Are professional and appropriate
    - Match the expected response type
    """

    def __init__(self):
        super().__init__("response_validator_agent")

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process method required by BaseAgent interface.
        Delegates to validate_response with proper parameter extraction.
        """
        return self.validate_response(
            input_query=input_data.get("input_query", ""),
            generated_response=input_data.get("generated_response", ""),
            response_type=input_data.get("response_type", "unknown"),
            extracted_data=input_data.get("extracted_data"),
            missing_fields=input_data.get("missing_fields"),
            expected_action=input_data.get("expected_action")
        )

    def validate_response(
        self,
        input_query: str,
        generated_response: str,
        response_type: str,
        extracted_data: Optional[Dict[str, Any]] = None,
        missing_fields: Optional[List[str]] = None,
        expected_action: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Validate a generated response against the input query.
        
        Args:
            input_query: The original customer query/email
            generated_response: The response generated by the model
            response_type: Type of response (clarification, confirmation, acknowledgment, etc.)
            extracted_data: Data extracted from the query (optional)
            missing_fields: Fields that were missing (optional)
            expected_action: Expected action that should have been taken (optional)
        
        Returns:
            Dictionary containing validation results with scores and feedback
        """
        try:
            print(f"ðŸ” RESPONSE_VALIDATOR: Validating response against input query...")
            print(f"   Response Type: {response_type}")
            print(f"   Query Length: {len(input_query)} characters")
            print(f"   Response Length: {len(generated_response)} characters")
            
            if not self.client:
                print(f"   âš ï¸ LLM not loaded, attempting to load...")
                self.load_context()
            
            if not self.client:
                print(f"   âŒ LLM not available, using fallback validation")
                return self._fallback_validation(input_query, generated_response, response_type)
            
            # Create validation prompt
            prompt = self._create_validation_prompt(
                input_query, generated_response, response_type,
                extracted_data, missing_fields, expected_action
            )
            
            # Define validation schema
            function_schema = {
                "type": "function",
                "function": {
                    "name": "validate_response",
                    "description": "Validate that the generated response accurately addresses the input query",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "relevance_score": {
                                "type": "number",
                                "minimum": 0.0,
                                "maximum": 1.0,
                                "description": "How well the response addresses the query (0-1)"
                            },
                            "accuracy_score": {
                                "type": "number",
                                "minimum": 0.0,
                                "maximum": 1.0,
                                "description": "Accuracy of information in the response (0-1)"
                            },
                            "completeness_score": {
                                "type": "number",
                                "minimum": 0.0,
                                "maximum": 1.0,
                                "description": "How complete the response is (0-1)"
                            },
                            "professionalism_score": {
                                "type": "number",
                                "minimum": 0.0,
                                "maximum": 1.0,
                                "description": "Professionalism and tone of the response (0-1)"
                            },
                            "overall_score": {
                                "type": "number",
                                "minimum": 0.0,
                                "maximum": 1.0,
                                "description": "Overall quality score (0-1)"
                            },
                            "is_valid": {
                                "type": "boolean",
                                "description": "Whether the response is valid and acceptable"
                            },
                            "issues": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "List of issues found in the response"
                            },
                            "strengths": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "List of strengths in the response"
                            },
                            "suggestions": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Suggestions for improvement"
                            },
                            "query_addressed": {
                                "type": "boolean",
                                "description": "Whether the response addresses the customer's query"
                            },
                            "missing_information": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Information that should be in the response but is missing"
                            },
                            "reasoning": {
                                "type": "string",
                                "description": "Detailed reasoning for the validation scores"
                            }
                        },
                        "required": [
                            "relevance_score", "accuracy_score", "completeness_score",
                            "professionalism_score", "overall_score", "is_valid",
                            "issues", "strengths", "suggestions", "query_addressed",
                            "missing_information", "reasoning"
                        ]
                    }
                }
            }
            
            # Make LLM call with deterministic settings (use OpenAI client for function calling)
            client = self.get_openai_client()
            if not client:
                return {"error": "OpenAI client not available for function calling"}
            
            response = client.chat.completions.create(
                model=self.config.get("model_name"),
                messages=[{"role": "user", "content": prompt}],
                tools=[function_schema],
                tool_choice={"type": "function", "function": {"name": "validate_response"}},
                temperature=0.0,  # Deterministic validation
                top_p=0.0,
                max_tokens=1000
            )
            
            if response.choices and response.choices[0].message.tool_calls:
                tool_call = response.choices[0].message.tool_calls[0]
                validation_result = json.loads(tool_call.function.arguments)
                
                print(f"âœ… RESPONSE_VALIDATOR: Validation complete")
                print(f"   Overall Score: {validation_result.get('overall_score', 0.0):.2f}")
                print(f"   Is Valid: {validation_result.get('is_valid', False)}")
                print(f"   Issues: {len(validation_result.get('issues', []))}")
                
                return {
                    "status": "success",
                    "validation_result": validation_result,
                    "timestamp": self._get_timestamp()
                }
            else:
                return self._fallback_validation(input_query, generated_response, response_type)
                
        except Exception as e:
            logger.error(f"Response validation failed: {e}")
            print(f"   âŒ Validation failed: {e}")
            return self._fallback_validation(input_query, generated_response, response_type)
    
    def _create_validation_prompt(
        self,
        input_query: str,
        generated_response: str,
        response_type: str,
        extracted_data: Optional[Dict[str, Any]],
        missing_fields: Optional[List[str]],
        expected_action: Optional[str]
    ) -> str:
        """Create validation prompt for LLM."""
        extracted_info = ""
        if extracted_data:
            extracted_info = f"\n\nEXTRACTED DATA FROM QUERY:\n{json.dumps(extracted_data, indent=2)}"
        
        missing_info = ""
        if missing_fields:
            missing_info = f"\n\nMISSING FIELDS THAT SHOULD BE ADDRESSED:\n{', '.join(missing_fields)}"
        
        expected_info = ""
        if expected_action:
            expected_info = f"\n\nEXPECTED ACTION: {expected_action}"
        
        return f"""
You are an expert response validator for a logistics customer service AI system.

Your task is to validate that the generated response accurately addresses the customer's query.

INPUT QUERY (Customer's Email):
{input_query}

GENERATED RESPONSE:
{generated_response}

RESPONSE TYPE: {response_type}
{extracted_info}
{missing_info}
{expected_info}

VALIDATION CRITERIA:

1. **RELEVANCE**: Does the response directly address the customer's query?
   - Does it answer their questions?
   - Does it address their concerns?
   - Is it on-topic?

2. **ACCURACY**: Is the information in the response accurate?
   - Are shipment details correct?
   - Are port codes/names accurate?
   - Are dates and numbers correct?
   - Is the extracted data properly reflected?

3. **COMPLETENESS**: Is the response complete?
   - Does it include all necessary information?
   - If clarification is needed, are all missing fields listed?
   - If confirmation is requested, are all details presented?
   - Are next steps clearly stated?

4. **PROFESSIONALISM**: Is the response professional and appropriate?
   - Is the tone appropriate for customer service?
   - Is it polite and respectful?
   - Is it clear and easy to understand?
   - Does it follow business communication standards?

5. **RESPONSE TYPE MATCHING**: Does the response match the expected type?
   - Clarification: Should ask for missing information clearly
   - Confirmation: Should present extracted data for customer confirmation
   - Acknowledgment: Should acknowledge receipt and next steps
   - Escalation: Should indicate escalation to sales team

VALIDATION INSTRUCTIONS:

1. Score each criterion from 0.0 to 1.0
2. Calculate overall score as weighted average (relevance: 30%, accuracy: 30%, completeness: 25%, professionalism: 15%)
3. Mark as valid if overall_score >= 0.7 and all critical criteria are met
4. List specific issues found
5. List strengths of the response
6. Provide actionable suggestions for improvement
7. Check if the query is properly addressed
8. Identify any missing information that should be included

Be thorough and specific in your validation. The goal is to ensure high-quality, accurate responses that properly serve customers.
"""
    
    def _fallback_validation(
        self,
        input_query: str,
        generated_response: str,
        response_type: str
    ) -> Dict[str, Any]:
        """Fallback validation when LLM is not available."""
        # Basic heuristics
        query_length = len(input_query)
        response_length = len(generated_response)
        
        # Check if response is too short
        completeness_score = min(1.0, response_length / max(query_length * 0.5, 100))
        
        # Check if response contains common professional phrases
        professional_phrases = ["thank you", "please", "best regards", "dear"]
        professionalism_score = sum(1 for phrase in professional_phrases if phrase.lower() in generated_response.lower()) / len(professional_phrases)
        
        # Basic relevance check (response should not be empty)
        relevance_score = 1.0 if response_length > 50 else 0.5
        
        overall_score = (relevance_score * 0.3 + completeness_score * 0.3 + professionalism_score * 0.4)
        
        return {
            "status": "fallback",
            "validation_result": {
                "relevance_score": relevance_score,
                "accuracy_score": 0.7,  # Default when we can't verify
                "completeness_score": completeness_score,
                "professionalism_score": professionalism_score,
                "overall_score": overall_score,
                "is_valid": overall_score >= 0.6,
                "issues": [] if overall_score >= 0.6 else ["Fallback validation - LLM not available"],
                "strengths": ["Response generated"],
                "suggestions": ["Enable LLM for detailed validation"],
                "query_addressed": response_length > 50,
                "missing_information": [],
                "reasoning": "Fallback validation using basic heuristics"
            },
            "timestamp": self._get_timestamp()
        }
    
    def _get_timestamp(self) -> str:
        """Get current timestamp."""
        from datetime import datetime
        return datetime.utcnow().isoformat()

